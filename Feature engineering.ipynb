{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTTFLERIXmYIPaI0hHi4K8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. What is a parameter?\n","  - A **parameter** is a variable or value that helps define and control the behavior of a function, system, or model. In programming, parameters are used to pass information to functions, allowing them to perform specific tasks based on given inputs.\n","\n","\n","2.  What is correlation? What does negative correlation mean?\n","  - **Correlation** is a statistical measure that describes the relationship between two variables. It indicates how one variable changes in relation to another—whether they move together (positive correlation), in opposite directions (negative correlation), or show no connection (zero correlation).\n","  - **Negative correlation** means that as one variable increases, the other decreases. In other words, they move in opposite directions. For example, in economics, higher unemployment rates often correlate negatively with consumer spending—when job losses rise, spending tends to drop.\n","\n","\n","3. Define Machine Learning. What are the main components in Machine Learning?\n","  - **Machine Learning (ML)** is a subset of artificial intelligence (AI) that enables computers to learn from data and improve their performance on a task without being explicitly programmed. It uses algorithms to identify patterns, make predictions, and optimize decisions based on experience.\n","\n","  1. **Data** – The foundation of ML; high-quality, structured data is crucial for training models.\n","  2. **Features** – Characteristics or attributes of data that influence predictions.\n","  3. **Algorithms** – Mathematical models that process data and learn patterns.\n","  4. **Model** – The trained system that makes predictions or classifications.\n","  5. **Training** – The process where the model learns from data using algorithms.\n","  6. **Evaluation** – Assessing model performance using metrics like accuracy or precision.\n","  7. **Deployment** – Integrating the trained model into real-world applications for decision-making.\n","\n","\n","\n","4. How does loss value help in determining whether the model is good or not?\n","  - Loss value measures how far a model's predictions are from the actual values. A lower loss indicates better accuracy, meaning the model is learning well. A higher loss suggests errors, requiring adjustments like better data, optimized parameters, or different algorithms to improve performance. It helps in evaluating and refining models for better predictions.\n","\n","\n","\n","5.  What are continuous and categorical variables?\n","  - **Continuous variables** are numerical and can take any value within a range. They are measurable and have infinite possible values, such as height, weight, or temperature.\n","\n","  - **Categorical variables** represent distinct groups or categories. They are non-numerical and include labels like colors, types of food, or gender.\n","\n","\n","\n","6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n","  - Handling **categorical variables** in Machine Learning is crucial because models typically work with numerical data. Here are some common techniques to transform categorical variables into a format suitable for ML algorithms:\n","\n","\n","  1. **Label Encoding** – Assigns unique numerical values to each category (e.g., \"Red\" = 0, \"Blue\" = 1).\n","  2. **One-Hot Encoding** – Creates binary columns for each category (e.g., \"Red\" → [1, 0, 0], \"Blue\" → [0, 1, 0]).\n","  3. **Ordinal Encoding** – Assigns numbers based on order/rank (e.g., \"Low\" = 1, \"Medium\" = 2, \"High\" = 3).\n","  4. **Binary Encoding** – Converts categories into binary numbers to reduce dimensionality.\n","  5. **Target Encoding** – Replaces categories with the mean of their corresponding target values (used in supervised learning).\n","  6. **Frequency/Count Encoding** – Represents categories using their frequency in the dataset.\n","  7. **Embedding Layers (for Deep Learning)** – Maps categories to dense vector representations.\n","\n","  Each method has its pros and cons, depending on the dataset and model used.\n","\n","\n","\n","7.  What do you mean by training and testing a dataset?\n","  - **Training a dataset** means using it to teach a machine learning model by adjusting its parameters to learn patterns and make accurate predictions.\n","\n","  - **Testing a dataset** involves evaluating the trained model on unseen data to check its performance and ensure it generalizes well to new inputs.\n","\n","\n","\n","8. What is sklearn.preprocessing?\n","  - sklearn.preprocessing is a module in Scikit-learn that provides tools for transforming raw data into a format suitable for machine learning models. It includes methods for scaling, normalizing, encoding categorical variables, and feature engineering to improve model performance.\n","\n","  - **StandardScaler** – Scales data to have a mean of 0 and variance of 1.\n","  - **MinMaxScaler** – Normalizes features within a fixed range (e.g., 0 to 1).\n","  - **LabelEncoder** – Converts categorical labels into numerical values.\n","  - **OneHotEncoder** – Converts categorical data into binary columns.\n","  - **PolynomialFeatures** – Creates interaction terms for feature expansion.\n","\n","  \n","\n","\n","\n","9. What is a Test set?\n","  - A Test set is a portion of a dataset used to evaluate a trained machine learning model. It contains unseen data that helps measure how well the model generalizes to new inputs. A good performance on the test set indicates a reliable model.\n","\n","\n","\n","10. How do we split data for model fitting (training and testing) in Python?\n"," How do you approach a Machine Learning problem?\n","  - Splitting Data for Model Fitting in Python:\n","    To split data into training and testing sets, we use train_test_split from Scikit-learn (sklearn.model_selection). This helps in evaluating model performance effectively.\n","\n","    from sklearn.model_selection import train_test_split\n","\n","   Sample data (X: features, y: target)\n","  X = [[1], [2], [3], [4], [5], [6]]\n","  y = [10, 20, 30, 40, 50, 60]\n","\n","   Splitting data (80% training, 20% testing)\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  print(\"Training set:\", X_train, y_train)\n","  print(\"Testing set:\", X_test, y_test)\n","\n","  test_size = 0.2 means 20% data goes into testing.\n","  random_state = 42 ensures reproducibility of results.\n","\n","  - Approach to Solving a Machine Learning Problem:\n","  A structured approach helps in building an effective ML model. Here’s a general workflow:\n","  1. **Define the Problem**  \n","   - Understand the goal: Classification? Regression? Clustering?\n","   - Identify the target variable.\n","\n","  2. **Collect & Preprocess Data**  \n","    - Gather reliable datasets.\n","    - Handle missing values, duplicates, and outliers.\n","    - Perform feature selection and engineering.\n","\n","  3. **Explore & Visualize Data**  \n","    - Generate summary statistics.\n","    - Use visualizations (histograms, scatter plots, correlations).\n","\n","  4. **Choose and Train a Model**  \n","    - Select a suitable algorithm (Linear Regression, Decision Trees, Neural Networks, etc.).\n","    - Split data for **training** and **testing**.\n","    - Fit the model on the training data.\n","\n","  5. **Evaluate the Model**  \n","    - Use metrics like accuracy, precision, recall, RMSE.\n","    - Tune hyperparameters using techniques like **Grid Search** or **Cross-validation**.\n","\n","  6. **Deploy the Model**  \n","    - Integrate the model into applications.\n","    - Monitor performance on real-world data.\n","\n","  7. **Improve & Iterate**  \n","    - Continuously refine based on feedback and new data.\n","    - Optimize with better feature selection or alternative algorithms.\n","\n","\n","\n","\n","11.  Why do we have to perform EDA before fitting a model to the data?\n","  - **Exploratory Data Analysis (EDA)** helps understand the dataset before fitting a model. It reveals patterns, anomalies, missing values, correlations, and feature distributions, ensuring the data is clean and well-structured. Proper EDA improves model accuracy, prevents errors, and helps select the best features, making the ML pipeline more efficient.\n","\n","\n","\n","\n","\n","14. How can you find correlation between variables in Python?\n","  - in Python, we can correlation between variables using Pandas or Numpy.\n","\n","  Using Pandas:\n","\n","  import pandas as pd\n","\n","  data = {'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]}\n","  df = pd.DataFrame(data)\n","\n","  correlation_matrix = df.corr()\n","  print(correlation_matrix)\n","\n","\n","  Using Numpy:\n","\n","  import numpy as np\n","\n","  x = [1, 2, 3, 4, 5]\n","  y = [10, 20, 30, 40, 50]\n","\n","  correlation = np.corrcoef(x, y)\n","  print(correlation)\n","\n","  These methods return the correlation coefficient, with values ranging from -1 to 1:\n","\n","  +1 → Strong positive correlation\n","\n","  -1 → Strong negative correlation\n","\n","  0 → No correlation\n","\n","\n","\n","15.  What is causation? Explain difference between correlation and causation with an example.\n","  - **Causation** means that one event directly causes another to happen. In other words, a change in one variable leads to a change in another.\n","\n","  Difference Between Correlation and Causation:\n","  - **Correlation** shows that two variables are related but does not imply that one causes the other.  \n","  - **Causation** confirms that one variable directly affects another.\n","\n","  Example:  \n","  - **Correlation:** Ice cream sales increase when swimming pool drownings rise. (Both happen more in summer, but ice cream does not cause drownings!)  \n","  - **Causation:** Drinking contaminated water causes food poisoning.\n","\n","\n","\n","16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n","  - An optimizer is an algorithm that adjusts a machine learning model's parameters (weights) to minimize the loss function and improve accuracy. It plays a crucial role in training deep learning models by refining predictions through iterative adjustments.\n","\n","\n","  Types or Optimizers with Examples:\n","  1. **Gradient Descent** – Basic optimization method that updates weights by computing the gradient of the loss function.  \n","   - Example: Used in Linear Regression to find the best-fit line.\n","\n","  2. **Stochastic Gradient Descent (SGD)** – Updates weights using a single random data point instead of the whole dataset, making training faster.  \n","    - Example: Used in image classification models like CNNs.\n","\n","  3. **Momentum** – Enhances **SGD** by adding velocity, helping the optimizer move past local minima and stabilize learning.  \n","    - Example: Used in deep learning architectures like ResNet.\n","\n","  4. **Adam (Adaptive Moment Estimation)** – Combines **momentum** and **RMSprop**, dynamically adjusting learning rates for better convergence.  \n","    - Example: Commonly used in deep learning models for NLP and computer vision.\n","\n","  5. **RMSprop (Root Mean Square Propagation)** – Maintains an adaptive learning rate to speed up training and prevent instability.  \n","    - Example: Used in reinforcement learning applications.\n","\n","\n","\n","17. What is sklearn.linear_model ?\n","  - sklearn.linear_model is a module in Scikit-learn that provides linear models for regression and classification tasks. It includes algorithms like Linear Regression, Logistic Regression, Ridge, Lasso, and Elastic Net, which help in predicting continuous values or classifying data based on features.\n","\n","\n","\n","18. What does model.fit() do? What arguments must be given?\n","  - The fit() function trains a machine learning model by learning from the given data. It adjusts the model’s parameters based on the input features and target values to make accurate predictions.\n","\n","  Arguments:\n","  x_train - feature/input data used for training.\n","  y_train - target/output data corresponding to x_train.\n","\n","\n","\n","19. What does model.predict() do? What arguments must be given?\n","  - The predict() function is used after training a model to make predictions on new or unseen data based on learned patterns.\n","\n","  Argumetns:\n","  X_test - input feature for which predictions are needed.\n","\n","\n","\n","20. What are continuous and categorical variables?\n","  - Continuous variables are numerical values that can take any range within a given limit. They are measurable and can have infinite possible values, such as height, temperature, or time.\n","\n","  - Categorical variables represent distinct groups or labels rather than numerical measurements. They define categories like colors, types of cars, or customer preferences.\n","\n","\n","\n","21. What is feature scaling? How does it help in Machine Learning?\n","  - **Feature Scaling in Machine Learning**  \n","  **Feature scaling** is a preprocessing technique that standardizes or normalizes numerical data, ensuring all features have a consistent range. Since different features may have varying scales, scaling helps models process them effectively.\n","\n","  How It Helps:  \n","  - **Improves Model Accuracy** – Prevents bias from large-valued features.\n","  - **Boosts Convergence Speed** – Speeds up training, especially for gradient-based models.\n","  - **Enhances Performance in Distance-Based Models** – Essential for algorithms like k-NN and k-Means clustering.\n","\n","\n","\n","\n","22.  How do we perform scaling in Python?\n","  - Feature scaling can be done using Scikit-learn's preprocessing module. The two most common techniques are:\n","  \n","  1. Standardization\n","\n","  from sklearn.preprocessing import StandardScaler\n","  scaler = StandardScaler()\n","  X_scaled = scaler.fit_transform(X)\n","\n","\n","  2. Min-Max Scaling\n","\n","  from sklearn.preprocessing import MinMaxScaler\n","  scaler = MinMaxScaler()\n","  X_scaled = scaler.fit_transform(X)\n","\n","\n","\n","\n","24. Explain data encoding?\n","  - Data encoding transforms categorical variables into numerical values so machine learning models can process them effectively.\n","\n","  Common Encoding Techniques:\n","\n","  1. **Label Encoding** – Assigns a unique number to each category (e.g., \"Red\" → 0, \"Blue\" → 1).  \n","  2. **One-Hot Encoding** – Creates binary columns for each category (e.g., \"Red\" → [1, 0, 0], \"Blue\" → [0, 1, 0]).  \n","  3. **Ordinal Encoding** – Assigns ordered numerical values based on ranking (e.g., \"Low\" → 1, \"Medium\" → 2, \"High\" → 3).  \n","  4. **Target Encoding** – Uses the mean of a category’s target variable as its encoded value.  \n","  5. **Binary Encoding** – Converts categories into binary digits to reduce dimensionality.\n","\n"],"metadata":{"id":"7kYzyuo9lMl4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxMZFoWXlFEF"},"outputs":[],"source":[]}]}